
<html>

<head>
      <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
      <style>
        table {
            border-collapse: collapse;
            width: 50%;
            margin: 20px;
        }
        table, th, td {
            border: 1px solid black;
        }
        th, td {
            padding: 8px;
            text-align: center;
        }
      </style>
      <title>CS 747: Programming Assignment 3 </title>
</head>

<body data-new-gr-c-s-check-loaded="14.1073.0" data-gr-ext-installed="">
      <br>

      <center>
            <h2>
                  CS 747: Programming Assignment 3 <br>
                  Optimal Cue-stick Control
            </h2>
            <h3>Total marks: 15</h3>
            <h3>(Prepared by Harsh Shah and Shrey Modi)</h3>
      </center>
      <br>
      <p style="text-align:center;">
        <img src="/images/sim_img.png" alt="Simulator image" width="750" height="400"> 
      </p>
      <p>In this assignment you will be doing...blah...blah...blah. Are you tired of this opening of assignments? Let's try with a different one. <br><br> Are YOU tired of missing shots while playing billiards? Do YOU want to beat Ronnie O' Sullivan in billiards? Then this assignment is an opportunity in disguise, wherein you get to make your own agent that is able to perform optimally in the game of billiards! </p>

	<p> <b>10k feet view</b> : You are given a simulator that simulates billiards for you (thanks to <a href="https://github.com/max-kov/pool">max-kov</a>) and you need to code an agent that decides an optimal action given the current state of the pool table (ball positions and hole positions). </p>


<h3>Code Structure</h3>
To access the assignment, clone <a href="#">this</a> github repo
 (or download compressed directory). The repo consists of the entire code required for this assignment. After cloning/extracting you should be able to get this directory structure:
 <p style="text-align:left;">
  <img src="/images/code_structure.png" alt="Code Structure" height="auto" width="30%"> 
</p>
Look up, "Running the Code" section to get the simulator running. 
You only have to modify and submit agent.py file for this assignment. The directory <code>traces</code> will consist of trace files of your actions, if you wish to store (more details in "Command-line arguments"). 
<code>config.py</code> file has the data regarding the configuration of table and the simulator. You might need table_margin, ball_radius, hole_radius, etc. Do NOT modify any files other than <code>agent.py</code>. 
Direct 0 marks if you change any variables (even dynamically) in <code>config.py</code> through <code>agent.py</code>.

<h3>Task</h3>
<h4>Original Billiards (can skip if you know the game)</h4>
Billiards is a cue sport that is typically played on a rectangular table covered with a smooth cloth, usually green, and surrounded by cushioned rails. The objective of the game is to score points by using a cue stick to strike and pocket a set of colored balls in a specific order or manner, depending on the variant of billiards being played. Here's an overview of the basic components and rules of the game:
<br>
<b>Components</b>:
<ol>
<li><b>Billiard Table</b>: The standard billiard table is typically 9 feet in length, although smaller sizes are also available for casual play. The table is covered with a smooth and often green cloth, designed to facilitate the smooth movement of balls.</li>
<li><b>Balls</b>: Billiards is played with a set of 16 balls. These include one white cue ball, 15 colored object balls, and these are usually divided into red and yellow (or sometimes solid and striped) balls. The object balls are numbered from 1 to 15.</li>
<li><b>Cue Stick</b>: Players use a long, slender cue stick to strike the cue ball and direct it towards the object balls.</li>
</ol>

<b>Basic Rules:</b>
<ol>
<li> <b>Objective</b>: The primary objective of the game varies depending on the specific variant being played, but in most cases, players aim to score points by pocketing the object balls in a predetermined order.</li>
<li> <b>Scoring</b>: In most billiard games, the player earns points by pocketing object balls. Each ball has a specific point value, typically from 1 to 7, with the higher-numbered balls being more valuable. The player who scores the most points by the end of the game wins.</li>
<li> <b>Order of Play</b>: Players take turns to shoot, with the player who successfully pockets an object ball getting to continue their turn. They must declare their intended shot, which ball they will target, and where they intend to pocket it.</li>
<li> <b>Fouls</b>: Various rules exist for fouls, such as failing to pocket an object ball, pocketing the cue ball, not hitting an object ball, and more. Fouls result in the loss of points or additional turns for the opponent.</li>
<li> <b>Winning</b>: The game can be won in different ways depending on the specific variant. Common ways to win include reaching a predetermined point total, pocketing all of the object balls in order, or achieving other specific objectives outlined in the rules.</li>
</ol>
Notable variants of billiards include "Eight-Ball," "Nine-Ball," "Straight Pool," and "Snooker," each with its own unique rules and scoring systems. Billiards is a strategic and skill-based game that requires precise cue ball control, planning, and an understanding of angles and ball physics.
Billiards is often enjoyed as both a competitive sport and a recreational activity in various settings, including pool halls, bars, and homes. It can be played by people of all skill levels, from casual players to highly skilled professionals.

<h4>Billiards for this assignment (might want to read this)</h4>
The earlier was what ChatGPT said...but we have made some modifications to the game. In this assingment, there is only one player (your agent)
and there is only one type of ball (solids)...not even 8 ball. If you pot the cue ball, the cue ball appears back to its initial position,
and rest of the board state remains unchanged. There are levels cooked by our chefs (TAs), and each level has a 
pre-defined maximum times you can hit the cue ball. In order to pass a level you need to pot all the colored balls within the maximum
tries limit. <br>
The configuration of the table can be found in <code>config.py</code> file of the given directory. You are allowed to access any
variables of <code>config.py</code> but not modify them (0 marks if we find any such case). <code>pygame</code> is used for the simulation
the game. Some details regarding how the simulation works:
<ol>
<li>In main.py, after game initialization, it is checked whether the board is stationary or not (game.all_not_moving())</li>
<li>If the game state is stationary, then a action is drawn from your agent using calls to cue (game.cue.cue_is_active) </li>
<li>After the force and angle is provided by agent, the simulator makes incremental updates to the states, and checks for collisions/ball pots at each increment (collisions.resolve_all_collisions)</li>
<li>Due friction, the balls stop moving after some iterations and the next action is fetched from the agent</li>
</ol>    

<h3>Details</h3>
<p>You need to code your agent in the file <i>agent.py</i>. The agent class has the following functions already defined -
   <code>set_holes()</code>, <code>action()</code>. You would only need to change <code>action()</code> which has only a single 
   argument <code>ball_pos</code> containing the state of the game, represented by positions of balls on the table.</p>
<p>Below is the default random agent given to you (in <code>agent.py</code>) :</p>
<code>
  <pre>
    class Agent:
    def __init__(self, table_config) -> None:
        self.table_config = table_config
        self.prev_action = None
        self.curr_iter = 0
        self.state_dict = {}
        self.holes =[]
        self.ns = utils.NextState()

    def set_holes(self, holes_x, holes_y, radius):
        for x in holes_x:
            for y in holes_y:
                self.holes.append((x[0], y[0]))
        self.radius = radius


    def action(self, ball_pos=None):
        return (2*random.random() - 1, random.random())
  </pre>
</code>
<!-- Describe what states what the agent gets, how the simulator, Exaplain the simualtion code and pygame-->
<p><b>What are states?</b> <code>ball_pos</code> (which is the state provided to agent) is a dictionary of the following form:-</p>
<code>
  <pre>
    {
      "0": [
          148.13247611407394,
          133.29004019051328
      ],
      "1": [
          920.0,
          80.0
      ],
      "2": [
          920.0,
          420.0
      ],
      "white": [
          148.13247611407394,
          133.29004019051328
      ]
  }
  </pre>
</code>
<p><b>State description : </b>Here, value of the key "white" is the coordinates of the cue ball and similarly the values of keys ranging from 0 to 2 are the coordinates of all the balls (including cue ball, denoted by index 0) present on the table. Note that cue ball is represented both as key "white" and key "0". 
<b>The state given to the agent is when all the balls are stationary</b>. Although the simulation goes through intermediate states, since the agent cannot do anything before the 
balls are stationary, these are not given to the agent.</p>

<p><b>Cartesian plane</b> : Coordinates of the balls (as well any other objects, like holes) are in the cartesian plane with the origin lying at the top left of the table. The positive X-Axis is horizontally rightwards, while the positive Y-Axis is vertically downwards.</p>

<p><b>Hole positions</b> : You are also given the position of the holes and radius of balls via set_holes() which is called by the simulator before the call to action(). Don't change set_holes() if you plan on completing the assignment. Hence, you have access to self.holes in <code>action()</code> function, which is essentially a list of hole coordinates.

<p><b>Output of the agent (important, pay attention)</b> : The<code>action()</code>function has to return a tuple of normalized angle and force (angle in range [-1,1], force in range [0, 1]). Angle is measured from negative Y-Axis with clockwise being positive angle (yes...we wanted to make your lives harder). The angle is linearly normalized to [-1,1], eg, pi/2 -> 0.5; -0.75pi-> -0.75, etc. 
  Force is a direct indicative of the initial speed of the cue ball, with 1 representing the maximum force and 0 being the minimum. You would not want to give an unnecesarily high force due to an increased random error term in the angle, resulting from a high force. </p>

<p><b>Error in angle</b> : In order to mimic real scenarios where high force to cue ball cultivates to less control, we add noise to the angle provided by the agent. Hence, final_angle = angle_provided + error<br>
Here, error ~ N(mean = 0; var = increasingfunc(F)), where F is the force provided by the agent</p>

<h4>Levels</h4>
The game proceeds in a level format. There are 10 public levels. Each level has a different configuration of the initial board in terms of both the number of balls as well as their positions. 
The goal is to pot all the balls within a given max_tries, specific to the level. As the level progresses the max tries is decreased by 2, starting from 30 for level 0. An example output is of the form: </p>
<code>
  <pre>
    Running the agent on 1 runs
    MAX_TRIES : 22
    Level 4 passed
    Tries : 14 ; Score : 4

    #### Levels passed : 1 out of 1 ####
  </pre>
</code>
<table>
  <thead>
      <tr>
          <th>Level Number</th>
          <th>Number of Balls</th>
          <th>Marks</th>
          <th>Max Tries</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td>4</td>
          <td>0.4</td>
          <td>30</td>
      </tr>
      <tr>
          <td>1</td>
          <td>4</td>
          <td>0.4</td>
          <td>28</td>
      </tr>
      <tr>
          <td>2</td>
          <td>4</td>
          <td>0.4</td>
          <td>26</td>
      </tr>
      <tr>
          <td>3</td>
          <td>5</td>
          <td>0.6</td>
          <td>24</td>
      </tr>
      <tr>
          <td>4</td>
          <td>4</td>
          <td>0.6</td>
          <td>22</td>
      </tr>
      <tr>
          <td>5</td>
          <td>5</td>
          <td>0.6</td>
          <td>20</td>
      </tr>
      <tr>
          <td>6</td>
          <td>4</td>
          <td>0.6</td>
          <td>18</td>
      </tr>
      <tr>
          <td>7</td>
          <td>5</td>
          <td>0.8</td>
          <td>16</td>
      </tr>
      <tr>
          <td>8</td>
          <td>5</td>
          <td>0.8</td>
          <td>14</td>
      </tr>
      <tr>
          <td>9</td>
          <td>6</td>
          <td>0.8</td>
          <td>12</td>
      </tr>
  </tbody>
</table>

<h4>Other Utilities</h4>

<p><b>Next state</b> : We have provided a get_next_state() function in <code>utils.py</code> which can be used by by callin <code>self.ns.get_next_state()</code>. The function takes three arguments - state, action and a random seed (for deterministic error).
   The function returns the next state based on the action and the current state provided, without running on the actual simulation.
   Hence, the agent can try any arbitrary action on any arbitrary state to fetch the next state and then decide which action to choose and return.</p>

<p><b>Traces</b> : Using the --generate-traces command line argument, your actions and corresponding next state is recorded and saved in the traces directory as a JSON. This can be used for generating the dataset for training your agent from the trace files. Eg of a trace file:</p>
<code>
  <pre>
    {
      "1": {
          "action": [
              -0.07807628366854513,
              0.5867365604164771
          ],
          "state": {
              "0": [
                  449.6884066084521,
                  268.87292264519124
              ],
              "3": [
                  750.0,
                  420.0
              ],
              "1": [
                  750.0,
                  250.0
              ],
              "2": [
                  250.0,
                  80.0
              ],
              "white": [
                  449.6884066084521,
                  268.87292264519124
              ]
          }
      },
      "2": {
          "action": [
              -0.2491698187632554,
              0.2605640895047009
          ],
          "state": {
              "0": [
                  681.9780964821935,
                  121.37648736770723
      ......................
  </pre>
</code>
Here, for each try (the keys of parent dictionary), the action and the produced next state is stored.

<h4>Marking scheme</h4>
Levels 0 to Level 9 (10 levels) have marks as mentioned in the previous table. You receive marks only if you pot the colored balls 
in the level with number of tries less than "Max Tries" which is also mentioned in the table. <br>
<b>Important : A level ends, either when you pot all the colored balls of the level, or when you exceed "Max Tries" (each try corresponds the the action taken by the agent).</b><br>
Private testcases (levels) will also have similar marking scheme, and the configuration as well as difficulty would be similar.<br>
The simulation runs with a default seed of 73. You can test your agent with different seeds. <br>
<b>The seed to be used for evaluation is hidden, and need not be 73.</b> 


<h3>Command-line arguments</h3>
<p> <b>--level-x n</b>: will run particular level n.</p>
<p> <b>--level-all</b>: will run all levels in a sequential manner</p>
<p> <b>--generate-traces</b>: will save a trace file of each action taken and the correspondding next state</p>
<p> <b>--no-render</b>: will not display the graphics of the simulation</p>
<p> <b>--generate-stats</b>: to print stats related to the simulation, like mean tries, total time, etc.</p>




<h3>Running the Code </h3>
<p>In order to run the code, follow the below steps, 
<ol>
<li> Downoad the compressed directory from here, and decompress it
<li> Enter the directory.
<li> In your python environment, run <code>pip install -r requirements.txt</code>
<li> Enter the directory <code>pool</code>
<li> Run, <code>python main.py --level-all</code> (look for command-line arguments for other options)
</ol>


</p>
<p>By default the <code>agent.py</code> script has a random agent. You need to modify the agent.</p>


<h3>Evaluation </h3>

      <p>6 marks have been reserved for the public testcases (levels). 6 more marks have been reserved for the hidden testcases which will be similar to the public ones. The final 3 marks have been reserved for your report. (6+6+3=15)</p>

      <p>Both the public and private test cases have 10 levels. Marks of each level is described in the marking scheme.</p>

      <p>Unlike the previous assignments, you have been given a free hand to come up with your agent. Hence, we would like to see a clear presentation of your approach. 
        For unique and successful approaches, we don't mind requesting the professor for bonus marks ;).<br> 
        Include a file named <code>report.pdf</code> that spells out the ingredients of your solution, any intermediate experiments that may have guided your decisions,
         learning curves if you did use some form of learning/parameter tuning, and so on. If your report is not sufficiently clear and informative, you will stand to lose marks. </p>

<p>The TAs and instructor may look at your source code and notes to corroborate the results obtained by your program, and may also call you to a face-to-face session to explain your code.</p>

      <h3>Submission</h3>
      <p>You have to submit one tar.gz file with the name (roll_number).tar.gz. Upon extracting, it must produce a folder with your roll number as its name. It must contain a <code>report.pdf</code> - the report as explained above, and one code file: <code>agent.py</code>. You must also include a <code>references.txt</code> file if you have referred to any resources while working on this assignment (see the section on Academic Honesty on the course web page). </p>


<h3>Deadline and Rules</h3>
To be discussed

<h3>Bonus</h3>
Lets see how much you know yourself!<br>
Aside from the marks that you get in the assignment, your agent will also be given a score. All the submissions will be ranked according
to the score of the agent (ties broken using roll number), and you will be given a chance to guess your rank. If your guess lies in the range +-5 of 
your actual rank, you will receive an extra mark! Yes, you heard me right, 1 mark for your accurate guess. A form will be floated later for the same to accumulate your guesses.<br>
Know your agent score using <b>--agent-score</b> command line argument to main.py.<br>
Eg 1: Actual rank = 17; Guessed rank = 22  -> Correct guess (1 bonus mark awarded)<br>
Eg 2: Actual rank = 17; Guessed rank = 24  -> Incorrect guess (0 bonus mark awarded)

<center>
  <h2>
      Have fun!!
  </h2>
</center>
</body>

</html>
